1.安装jdk
把jdk.tar.gz解压在/usr/local目录下
编辑/ect/profile
JAVA_HOME=/usr/local/jdk1.8
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tool.jar
PATH=$PATH:JAVA_HOME/bin
2.安装scala
把jdk安装在/opt/下
编辑/etc/profile
SCALA_HOME=/opt/scala-2.12.3
3.安装spark
把spark安装在/opt/下
编辑/etc/profile
SPARK_HOME=/opt/spark-2.1.1-bin-hadoop2.7
PATH=$PATH:$SPARK_HOME/bin
export PATH
3.编辑conf下spark-env.sh
cp spark-env.sh.template spark-env.sh
#在里面加入配置
export SCALA_HOME=/opt/scala-2.12.3
export JAVA_HOME=/usr/local/jdk1.8.0_144
export SPARK_HOME=/opt/spark-2.1.1-bin-hadoop2.7
export SAPRK_EXECUTOR_MEMORY=1G
#新建slaves文件
cp slaves.template slaves
4.设置主机名
vi /etc/sysconfig/network
HOSTNAME=lele

vi /etc/hosts
在 127.0.0.1后面加上lele
5.单机测试spark
./bin/run-example SparkPi 10